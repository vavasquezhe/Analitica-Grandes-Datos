{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bedc811d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Repaso Hive\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ba9f63",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Analítica de Grandes Datos\n",
    "* ##### Facultad de Minas\n",
    "* ##### Universidad Nacional de Colombia\n",
    "* ##### Autor: Valentina Vásquez Hernandez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a667adc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 0. ¿Cómo ejecuto el código de este taller?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71ba33a",
   "metadata": {},
   "source": [
    "* **Paso 1.** Ejecutar la imagen de docker `jdvelasq/hive:2.3.9` [click aquí para ver el comando](https://jdvelasq.github.io/courses/analitica_de_grandes_datos/index.html)\n",
    "* **Paso 2.** Una vez se encuentren dentro de la imágen, pueden seguir cualquiera de las siguientes opciones: \n",
    "    > **Paso 2.1.** Clonar el repositorio de los talleres dentro de su máquina e inicializar jupyter [lab o notebook] en el directorio donde se encuentre este libro .ipynb\n",
    "    \n",
    "    > **Paso 2.2.** Guardar comandos en un archivo `.hql` y luego ejecutarlo con el comando `hive -f`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4b7730",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470876c2",
   "metadata": {},
   "source": [
    "### 1. ¿Qué es Apache Hive?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52efa1ad",
   "metadata": {},
   "source": [
    "Apache Hive es un sistema de almacen de datos *OpenSource* basado en Hadoop y que se ha especializado en operaciones de extracción, transformación y carga de datos (ETL, por sus siglas en ingles), facilitando el análisis de los mismos a gran escala. Es posible interactuar con Hive a través del command-line, Java™ Database Connectivity (JDBC) driver o Open Database Connectivity (ODBC) driver, usando lenguaje Hive query Language (HQL). [1](https://hive.apache.org/) [2](https://www.ibm.com/analytics/hadoop/hive) [3](https://aws.amazon.com/es/big-data/what-is-hive/)\n",
    " \n",
    " \n",
    "> **Arquitectura Hive** [4](https://www.javatpoint.com/hive-architecture)\n",
    "\n",
    "\n",
    "<img src=\"hivearq.jpg\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "\n",
    "Con respecto a `Pig` (Yahoo) y  lo que se ha explorado dentro de las funciones MapReduce, `Hive` (Facebook) presenta las siguientes ventajas: \n",
    "\n",
    "\n",
    "* Hive ofrece la ejecución de tareas MapReduce a través de un lenguaje SQL mientras que Pig lo hace a través de un lenguaje propio basado en el flujo de información\n",
    "* Hive es ampliamente usado para el análisis de dato, generación de reportes y soporta funciones OLAP.\n",
    "* Existen implementaciones comerciales que brindan soporte como Amazon EMR (Apache Spark, Apache Hive y Presto) y IBM Db2 Big SQL. La comunidad es considerablemente mas amplia.\n",
    "\n",
    "> **Haddop Ecosystem** [5](https://mdivk.gitbooks.io/hadoop-practice-for-beginners-with-illustration/content/appendix_10_the_hadoop_ecosystem_in_a_nutshell.html)\n",
    "\n",
    "<img src=\"haddopeco.jpg\" alt=\"drawing\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa35fb9",
   "metadata": {},
   "source": [
    "**Principales comandos:**\n",
    "\n",
    "Estructura básica\n",
    "\n",
    "> `SELECT` [*|fields] \n",
    "`FROM` table-name\n",
    "    [where condition] \n",
    "    [AND condition]\n",
    "\n",
    "Cargar datos locales al HDFS\n",
    "> `LOAD DATA LOCAL INPATH` 'local-file' `OVERWRITE INTO TABLE` table-name\n",
    "\n",
    "\n",
    "Guardar datos en una ubicación específica\n",
    "> `INSERT OVERWRITE DIRECTORY` 'output'\n",
    "`ROW FORMAT DELIMITED FIELDS TERMINATED BY` 'delimeter'\n",
    "-- QUERY SQL\n",
    "\n",
    "*Manual del lenguaje*: https://cwiki.apache.org/confluence/display/Hive//LanguageManual\n",
    "\n",
    "*Cheat Sheet (se encuentra descargado en este repo)*: http://hortonworks.com/wp-content/uploads/2016/05/Hortonworks.CheatSheet.SQLtoHive.pdf "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37306f78-df18-48b3-b3ac-b105986a8230",
   "metadata": {},
   "source": [
    "> **Particionamiento:** [6](https://www.javatpoint.com/hive-architecture)\n",
    "\n",
    "<img src=\"patitioning.jpg\" alt=\"drawing\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f74bdae",
   "metadata": {},
   "source": [
    "### 1. ¿Cómo uso Hive?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fadadc1",
   "metadata": {},
   "source": [
    "Es posible ejecutar Hive de manera local o usando el HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4827c73d-f519-478c-8fdd-246cd6577c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage ./hive <parameters> --service serviceName <service parameters>\n",
      "Service List: beeline cleardanglingscratchdir cli hbaseimport hbaseschematool help hiveburninclient hiveserver2 hplsql jar lineage llap llapdump llapstatus metastore metatool orcfiledump rcfilecat schemaTool version \n",
      "Parameters parsed:\n",
      "  --auxpath : Auxiliary jars \n",
      "  --config : Hive configuration directory\n",
      "  --service : Starts specific service/component. cli is default\n",
      "Parameters used:\n",
      "  HADOOP_HOME or HADOOP_PREFIX : Hadoop install directory\n",
      "  HIVE_OPT : Hive options\n",
      "For help on a particular service:\n",
      "  ./hive --service serviceName --help\n",
      "Debug help:  ./hive --debug --help\n"
     ]
    }
   ],
   "source": [
    "!hive --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7579b6-0ff6-414b-9d73-5f823d214187",
   "metadata": {},
   "source": [
    "A continuación se crea la tabla a usar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "221d23bc-6ae1-4c8b-b6cf-c55fda801d90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting create_table_cars.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile create_table_cars.hql\n",
    "\n",
    "DROP TABLE IF EXISTS cars;\n",
    "\n",
    "CREATE TABLE cars (\n",
    "    make STRING, \n",
    "    fuel_type STRING,\n",
    "    num_of_doors STRING,\n",
    "    length FLOAT,\n",
    "    width FLOAT, \n",
    "    price INT\n",
    ")\n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72764ea8-cb42-486c-a87c-cbdcb4161d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging initialized using configuration in jar:file:/opt/hive/lib/hive-common-2.3.9.jar!/hive-log4j2.properties Async: true\n",
      "OK\n",
      "Time taken: 7.999 seconds\n",
      "OK\n",
      "Time taken: 1.024 seconds\n"
     ]
    }
   ],
   "source": [
    "!hive -f create_table_cars.hql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30952f14-fe37-4a11-bc08-0b9335410d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting load_data_cars.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile load_data_cars.hql\n",
    "LOAD DATA LOCAL INPATH \"data/cars_subset.csv\" OVERWRITE INTO TABLE cars;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "318f0abe-f561-4844-913a-697969e623c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging initialized using configuration in jar:file:/opt/hive/lib/hive-common-2.3.9.jar!/hive-log4j2.properties Async: true\n",
      "Loading data to table default.cars\n",
      "OK\n",
      "Time taken: 9.385 seconds\n"
     ]
    }
   ],
   "source": [
    "!hive -f load_data_cars.hql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8df82949-3eb3-42c4-9fdf-af66d34a9cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting describe_cars.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile describe_cars.hql\n",
    "DESCRIBE cars;\n",
    "SELECT * FROM cars LIMIT 5;\n",
    "SHOW TBLPROPERTIES cars;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e686dbf3-529d-4f58-baab-91659c193c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging initialized using configuration in jar:file:/opt/hive/lib/hive-common-2.3.9.jar!/hive-log4j2.properties Async: true\n",
      "OK\n",
      "make                \tstring              \t                    \n",
      "fuel_type           \tstring              \t                    \n",
      "num_of_doors        \tstring              \t                    \n",
      "length              \tfloat               \t                    \n",
      "width               \tfloat               \t                    \n",
      "price               \tint                 \t                    \n",
      "Time taken: 8.253 seconds, Fetched: 6 row(s)\n",
      "OK\n",
      "audi\tgas\tfour\t176.6\t66.2\t13950\n",
      "audi\tgas\tfour\t176.6\t66.4\t17450\n",
      "audi\tgas\tfour\t192.7\t71.4\t17710\n",
      "audi\tgas\tfour\t192.7\t71.4\t23875\n",
      "bmw\tgas\ttwo\t176.8\t64.8\t16430\n",
      "Time taken: 2.184 seconds, Fetched: 5 row(s)\n",
      "OK\n",
      "numFiles\t1\n",
      "numRows\t0\n",
      "rawDataSize\t0\n",
      "totalSize\t5505\n",
      "transient_lastDdlTime\t1655221394\n",
      "Time taken: 0.078 seconds, Fetched: 5 row(s)\n"
     ]
    }
   ],
   "source": [
    "!hive -f describe_cars.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c45e1ee",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
